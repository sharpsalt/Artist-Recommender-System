# -*- coding: utf-8 -*-
"""RS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNNYTnXVuNrgr4CZaIvmEFg5wQFl8lPC
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

import os, sys, gc, warnings
import pandas as pd
import numpy as np
import plotly
import plotly.express as px
import plotly.graph_objs as go
import plotly.figure_factory as ff
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
import random
import implicit
import scipy.sparse as sparse

from tqdm.notebook import tqdm
from collections import defaultdict
from timeit import default_timer as timer
import time
from plotly import tools
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from IPython.display import display, HTML
from IPython.core.interactiveshell import InteractiveShell

from surprise import KNNWithMeans, KNNBasic, KNNWithZScore
from surprise import Dataset, accuracy, Reader
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cosine as cos
from scipy.sparse.linalg import spsolve
from sklearn.preprocessing import MinMaxScaler

# Warnings and display settings
warnings.filterwarnings("ignore")
InteractiveShell.ast_node_interactivity = "all"
pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.max_colwidth", None)
display(HTML(data="""<style>div#notebook-container {width:95%;}</style>"""))

# Plotting styles
sns.set_style("darkgrid")
plotly.offline.init_notebook_mode()
# %matplotlib inline
mpl.style.use('ggplot')
plt.style.use('fivethirtyeight')
sns.set(context='notebook', palette="dark", style="whitegrid", color_codes=True)

# Commented out IPython magic to ensure Python compatibility.
# %pip install "numpy<2.0"

df=pd.read_csv("usersha1-artmbid-artname-plays.tsv",delimiter="\t",header=None,usecols=[0,2,3],names=['userId','artistName','listens'])

#In collaborative filtering we purely focus on the relationship between user to item, how do we it,based on ratings

#in other type of recommendation system, we can use other logic too, what's the channel,etc like we can use more attriutes or features

#We go back and see the content -content based recommendation

df.head()

"""# Why we need to do some data exploration?"""

#Once we completes NLP we will get to know , how to generate content based Recommendations using NLP Technqiue

#For colaboritive filtering to work we alwyas need that matrix
#How much a user has liked that feature should be reflected in the matrix

#Ab humlog transpose krna pdega isko matrix me so
#we will use the pivot table for it....

x=df.artistName.unique()
print(x.size)
y=df.userId.unique()
print(y.size)

# so when we transpose it into matrix so matrix ka size x.size * y.size hoga so it is pretty large we can go run out of memory so we will reduce the size of matrix
#so for reducingthe size of the matrix we will drop some user...
#Maybe user who has listened to only 1 artists , so
#Like you can also aks me that how can we make recommendation to that user in future...
#but we still trying the that user, let's say we are dropping x user which listened to this particular artist 100 time and rest all of it is 0
#there could be many reason like wo naya users bhi hoskta or he/she may like that certain artists
#Such type p problems are called as Cold start Problem

"""# For a recommendation to works we need some data that replicated the behaviour of the user so if we don't have data for the user i have no clue that what behaviour this user exhibit isko hi Cold start problem bolte

#For dealing with such kind of problem we go with Popularity based Recommendation System,until we have enough data for this user to be part of collaborative filtering
till then we simply go with what kind of popularity like by chance if we know what location is of the user belongs to jo ki wo song sunraha hai...
"""

#so we can find out who's more popular artist in that region until we get to generate some data by this user we can
#marginally understand the behaviour of the user
#vice-versa the filters can be applied on the artists also...
#Let's say there's a new artist who is not actively making song but kabhi kabhi gaana likh deta hai so
#and you want to recommend those artists so we'll use the popularity based recommendar system...
#The artist would listens to few of things maybe we remove thos user and artist data rom our training process

#Here when we say consider we are inlcuding only users , not others demographsic
#For coldtsrat problem , demographics helps , we can use KNN for new customer and then do recommendations on it...

# Artist Summary
artist_sum = df.groupby('artistName').agg(
    {'userId':'nunique','listens':'sum'}
).reset_index()
artist_sum.columns = ['artistName', 'nU_listen', 'n_listens']
artist_sum = artist_sum.sort_values('nU_listen', ascending=False).reset_index(drop=True)

# Distribution plot
fig = px.histogram(artist_sum, x='nU_listen')
fig.update_traces(xbins=dict(start=0.0, end=150, size=1))
fig.update_xaxes(title_text="Number of users listening to an artist")
fig.update_yaxes(title_text="Count of artists (log scale)", type="log")
fig.update_layout(
    height=600, width=1200,
    bargap=0.01,
    template='plotly_dark',
    barmode='group',
    showlegend=True,
    margin=dict(l=50, r=50, t=50, b=50),
    font=dict(size=14, family="Courier New"),
    title=dict(
        font_size=20,
        text="How many users listen to a typical artist?",
        font_family="Arial",
        y=0.99,
        x=0.5,
        xanchor='center',
        yanchor='top'
    )
)

fig.show()

# Summary by users
user_sum = df.groupby('userId').agg({'artistName': 'nunique', 'listens': 'sum'}).reset_index()
user_sum.columns = ['userId', 'nA_listen', 'n_listens']
user_sum = user_sum.sort_values('nA_listen', ascending=False).reset_index(drop=True)
display(user_sum.head(3))

fig = px.histogram(user_sum, x="nA_listen", nbins=20)
fig.update_traces(xbins=dict(start=0.0, end=100, size=1))
fig.update_xaxes(title_text="Number of artists listened by a user")
fig.update_yaxes(title_text="Count of users (log scale)", type="log")
fig.update_layout(
    height=400, width=1200,
    bargap=0.01,
    template='plotly_dark',
    barmode='group',
    showlegend=True,
    margin=dict(l=50, r=50, t=50, b=50),
    font=dict(size=14, family="Courier New"),
    title=dict(
        font_size=20,
        text="How many artists are our users listening to on average?",
        font_family="Arial",
        y=0.99, x=0.5, xanchor='center', yanchor='top'
    )
)

fig.show()

def filter_lastfm_raw(df, user_sum, artist_sum, user_t=25, artist_t=25):
    # function that can filter users and artists based on thresholds
    # filter users and artists for train and test for modelling
    # The thresholds of how many artist are listened to & how many users listen an artist are used
    users_req = user_sum[user_sum['nA_listen']>=artist_t]['userId'].tolist()
    artists_req = artist_sum[artist_sum['nU_listen']>=user_t]['artistName'].tolist()

    df_main = df[:df.shape[0]].copy()
    df_main['user_flag'] = np.where(df_main['userId'].isin(users_req), True, False)
    df_main['artist_flag'] = np.where(df_main['artistName'].isin(artists_req), True, False)
    df_main['filter_user_artist_flag'] = np.where(df_main['user_flag'].values & df_main['artist_flag'].values, True, False)
    # print('Data remaining after filtering users and artists: ', df_main[df_main['filter_user_artist_flag']].shape[0], '/', df_main.shape[0])
    # print('# users after filtering: ', len(users_req), ', # Artists after filtering: ', len(artists_req))

    df_main = df_main[df_main['filter_user_artist_flag']][['userId', 'artistName', 'listens']]
    users_req = sorted(df_main['userId'].unique().tolist())
    artists_req = sorted(df_main['artistName'].unique().tolist())

    # Split happens here
    df_main['split'] = None
    test_idx = df_main.groupby("userId")["artistName"].sample(frac=0.3, random_state=2047).index.values
    df_main.loc[test_idx, 'split'] = 'Test'
    # print('fraction of test data:', len(test_idx)/df_main.shape[0])

    train_idx = df_main[~(df_main['split']=='Test')].index.values
    # train_idx = df_main[(df_main['filter_user_flag']==True)&(df_main['split']!='Test')].index.values
    df_main.loc[train_idx, 'split'] = 'Train'
    df_main = df_main.sort_values(['userId', 'split', 'listens'], ascending=[True, False, False]).reset_index(drop=True)
    gc.collect()

    return df_main, users_req, artists_req

#We are just doing this because for new users we don't know their behaviour and at the same time for new artist we can't judge more about them
#so we are trying to eliminate new artists and new users with less number of Interactions

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# df_filt,users_filt,artists_filt=filter_lastfm_raw(df,user_sum,artist_sum,user_t=150,artist_t=75)
# display(df_filt.head())
# print("Sample Users",users_filt[:5])
# print("Sample Artists",artists_filt[2037:2047])
# print(' ')

#Now when we split this data in such a way that after applying those filters some of these records would considered ino training and some of these records would go into testing

def plot_heat(df, title, x_col, y_col, w=75, h=25, color='Viridis'):
    # Helper function to plot heatmap
    z = df[x_col].values
    # z_text = np.around(z, decimals=2)  # Uncomment if you want text labels
    trace = go.Heatmap(
        z=z,
        x=x_col,
        y=df[y_col].astype('str'),
        xgap=0.1,
        ygap=0.1,
        colorscale=color
    )
    data = [trace]
    layout = go.Layout(
        autosize=False,
        width=len(x_col) * w,
        height=min(df[y_col].nunique() * h, 500),
        title=title,
        xaxis=dict(type='category', automargin=True),
        yaxis=dict(type='category', title=y_col, automargin=True)
    )
    fig = go.Figure(data=data, layout=layout)
    iplot(fig, filename='labelled-heatmap')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Sample users with <25 unique artists
# samp_users_plot = (
#     df_filt.groupby('userId')
#     .agg({'artistName': 'nunique'})
#     .reset_index()
# )
# df_split_plot = df_filt[
#     df_filt.userId.isin(
#         samp_users_plot[samp_users_plot.artistName < 25]['userId'][:5].tolist()
#     )
# ].reset_index(drop=True)
# df_split_plot['listens'] = np.where(df_split_plot['split'] == 'Train', 1, 0)
# df_split_plot['artistName'] = df_split_plot['artistName'].str[:10]
# # Pivot table for plotting
# df_split_plot = (
#     df_split_plot
#     .pivot_table(index='userId', columns='artistName', values='listens')
#     .reset_index()
# )
# # Heatmap plot
# plot_heat(
#     df_split_plot,
#     "Train test split for sample users - {'1': Train, '0': Test}",
#     [c for c in df_split_plot.columns if c not in ['userId']],
#     'userId',
#     w=15,
#     h=75
# )
# del samp_users_plot, df_split_plot
# gc.collect();
#

#This Yellow One's will be there in training and the other one would b in testing....

#Ab transpose krenge and get that sparse matrix , but when we transpose it will create a huge sparse matrix
#Why it creates a huge sparse matrix , remember in our original data we have 3.5L user through 2.9L artits even after filtering out we end with 1l*50k
#But this user might not have listened to 50K artists and only to 100 so only some terms would be there rest all are zeroes so

"""# Collaborative Filtering is broadly divided into two types
# 1)Memory Based Collaborative Filtering
#    1.1)User-Item
#    1.2)Item-Item
# 2)Model Based Collabortive Filtering
#    2.1)NMF(Non Negative Matrix Factorization)
#    2.2)Clustering
"""

#We can use Cosine Similarity,KNN (euclidean Based)

"""# Basically recommendation System Banane me 2 Stages Hote hai

# Model Building(Similarity-Ordering user based on Similarity)
# Recommendation Phase(Similarity Users-Pick top N Recommendation)
"""

#First Thing which we are goiing to build is User to user (We will use Cosine Similarity to find out similar user)
#and based on that we will be making our predictions

#Since this is user-item based filtering ,we can simply repute it by simply filling it with NA (for missing values )assuming that the person has not watched

#If it is KNN Based instead of COsine then we will use the mean of it and then fill it by its mean

#split it Train-test
#Pivot into user-item format
#Impute the missing values
#Compute pairwise imilairities
#Get recommendation based on similar users and weigh them

#sicnce this is memory and compute we shall use filtered data with at least 150 users and 75 artists threshold earlier
missing_inputs = False
clip_rating = False

# Filter the training and test data
df_train = df_filt[df_filt.split == 'Train'][['userId', 'artistName', 'listens']]
df_test = df_filt[df_filt.split == 'Test'][['userId', 'artistName', 'listens']]

if clip_rating:
    df_train['listens'] = df_train['listens'].clip(upper=1)

# Pivot to user-artist format (sampled to avoid heavy compute)
df_pivot = df_train[:100000].pivot_table(
    index='userId',
    columns='artistName',
    values='listens'
)

# Imputation
if missing_inputs:
    # Fill missing with user’s average listens
    df_train_imputed = df_pivot.T.fillna(df_pivot.mean(axis=1)).T
else:
    # Fill missing with 0 (unlistened artists)
    df_train_imputed = df_pivot.fillna(0)

# Compute pairwise cosine similarity between users
user_similarity = cosine_similarity(df_train_imputed.values)

# Remove self-similarity (set diagonal = 0)
np.fill_diagonal(user_similarity, 0)

print("Sample records from train set: ")
display(df_train.head(2))

print(' ')
print("Sample records from test set: ")
display(df_test.head(2))

print(' ')
print("Pivoted Dataset for user similarity Calculations: ")
display(df_pivot[102:105].iloc[:,100:110])

print(' ')
print("Imputed dataset for user similaity Calculations: ")
display(df_train_imputed[102:105].iloc[:,100:110])


print(' ')
print("User Similarity for a sample user")
user_similarity[1]

#Our Stage 1 is Completed (Model Building Phase) as we only generated our similarity
#Now we move to stage-2 where we will get the Recommendations user to user using the Collaborative Filtering

def get_rec_u2u_cb(user_index, similarity_matrix, df_user_item_pivot, df_p_imputed, df_test,
                   item_col='artistName', n_rec_user=100, n_rec_items=10):
    """
    Function to get similar users, unrated artists and scores for a sample user.

    Parameters
    ----------
    user_index : int
        Index of the target user in the pivot table.
    similarity_matrix : numpy.ndarray
        Precomputed user-user similarity matrix.
    df_user_item_pivot : pandas.DataFrame
        User-item pivot table.
    df_p_imputed : pandas.DataFrame
        Imputed ratings DataFrame (with missing values filled).
    df_test : pandas.DataFrame
        Test DataFrame to validate recommendations.
    item_col : str, default 'artistName'
        Item column name.
    n_rec_user : int, default 100
        Number of similar users to consider.
    n_rec_items : int, default 10
        Number of items to recommend.
    """

    # Get the user id from user index and pivoted frame
    user_c = df_user_item_pivot.index.values[user_index]

    # Sort all the similar users by index based on similarity matrix
    similar_user_index = np.argsort(similarity_matrix[user_index])[::-1]

    # Sort the said similar users' score
    similar_user_score = np.sort(similarity_matrix[user_index])[::-1]

    # Get all the unrated artists for this particular user
    unrated_artists = df_user_item_pivot.iloc[user_index][df_user_item_pivot.iloc[user_index].isna()].index

    # Weight ratings of top n similar listens & compute mean
    mean_artist_recommendations = (
        (df_p_imputed.iloc[similar_user_index[:n_rec_user]].T * similar_user_score[:n_rec_user])
        .T.mean(axis=0)
    )

    best_artist_recommendations = (
        mean_artist_recommendations[unrated_artists]
        .sort_values(ascending=False)
        .to_frame('pred')
        .reset_index()
    )

    rec_req = [
        user_c,
        best_artist_recommendations[:n_rec_items],
        df_test[df_test.userId == user_c][item_col].tolist()
    ]

    return rec_req

def pak(y_true, y_pred, k=10):
    """
    Function to compute Precision@k and Recall@k

    Parameters
    ----------
    y_true : list of lists
        Ground truth items for each user.
    y_pred : list of lists
        Predicted ranked items for each user.
    k : int, default 10
        Number of top predictions to consider.

    Returns
    -------
    precisions : list
        Precision@k values for each user.
    recalls : list
        Recall@k values for each user.
    """

    precisions = []
    recalls = []

    for i, pred in enumerate(y_pred):
        # True positives in top-k
        tp = len([p for p in pred[:k] if p in y_true[i]])

        # Precision@k
        precisions.append((tp * 100) / k if k > 0 else 0)

        # Recall@k
        recalls.append((tp * 100) / len(y_true[i]) if len(y_true[i]) > 0 else 0)

    return precisions, recalls

#In this collaborative filtering,the memory based approac we are blindly filtering and ignoring everything and only trusting user to item affifity and this affidity can be rating or something else

!pip install ipywidgets

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Take first 1000 users for evaluation
# user_indices_to_rec = df_pivot.index.values[:1000]
# 
# # Store all recommendations
# recs_u2u_all = []
# 
# # Generate recommendations for each user
# for i, u in enumerate(tqdm(user_indices_to_rec.tolist())):
#     rec_u = get_rec_u2u_cb(
#         i,
#         user_similarity,
#         df_pivot,
#         df_train_imputed,
#         df_test,
#         n_rec_user=100,
#         n_rec_items=100
#     )
#     recs_u2u_all.append(rec_u)
# 
# # Extract ground truth and predictions
# y_true_u2u = [i[2] for i in recs_u2u_all]
# y_pred_u2u = [i[1]['artistName'].values.tolist() for i in recs_u2u_all]
# 
# # Compute Precision@25 and Recall@25
# precisions_u2u, recalls_u2u = pak(y_true_u2u, y_pred_u2u, k=25)
# 
# # Store which users we evaluated
# users_to_rec = [i[0] for i in recs_u2u_all]
#

#There Two setps like once we compute the similairty we are trying to generate the first Top-N recommendationa and then we are trying to comoute the precision and recall

y_pred_u2u[0] #Ye top 100 recommendation hai

y_true_u2u

fig, ax = plt.subplots(1, 2, figsize=(12, 4))   # Create 1 row, 2 columns of plots
fig.suptitle("User-User CF", fontsize=16)       # Add a main title

# Plot distributions of precision and recall
sns.distplot(precisions_u2u, ax=ax[0])
sns.distplot(recalls_u2u, ax=ax[1])

# Set individual titles for subplots
ax[0].set_title('Precision by User')
ax[1].set_title('Recall by User')

fig.tight_layout()   # Adjust layout so plots don’t overlap


print('-- Metrics for User-User CF -- ')
print('Average precision:', np.mean(precisions_u2u))
print('Average recall:', np.mean(recalls_u2u))

#Precision and recall dekh ke i can say like it is not very best, we can still improve it a little bit....
#So Imporving it



#This is our Test Dataset, like suppose we have 100 users in test Dataset
#u1 a3 a5 a9 a10 a26
#u2 a27 a29 a31 a9 a23
#u3
.
.
.
#un

#Suppose for user1 i have made prediction,since we picked top-N element where top-N is 25
#we will be making 25 prediction
#There are 10 artists which user listens,only 3 we did cover so recall is 3/10

#For every user the precision at ka nd recall at k will be computed...
#Now if we average this out across the users
#Since this is a number so we plot it as distribution


#We are Picking top k users but they are not similar and this is the problem with recommendor system
#In that case the recommendation which we get won't be that good...

def get_sims_plot(user_idx, df_pivot, user_similarity, df_test, recs_u2u_all, n_sim=25):
    # Function to get similar users and artists

    user_c = df_pivot.index.values[user_idx]

    # Get most similar users
    similar_users_c = np.argsort(user_similarity[user_idx])[::-1]
    similar_user_score_c = np.sort(user_similarity[user_idx])[::-1]

    # Filter test set for similar users + target user
    df_plot_c = df_test[df_test.userId.isin(
        df_pivot.index.values[list(similar_users_c[:n_sim]) + [user_idx]]
    )]

    # Keep only artists recommended to this user
    df_plot_c = df_plot_c[df_plot_c.artistName.isin(recs_u2u_all[user_idx][2])]

    # Apply log transform to listening counts (avoid zeros by +1)
    df_plot_c['listens'] = np.log(df_plot_c['listens']) + 1

    # Pivot to userId × artistName matrix
    df_plot_c = df_plot_c.pivot_table(
        index='userId',
        columns='artistName',
        values='listens'
    ).fillna(0).reset_index()

    return df_plot_c, user_c
print('Users with top performance:', np.argsort([-i for i in precisions_u2u])[:10])
print('Users with low performance:', np.argsort([-i for i in precisions_u2u])[-10:])

#Upar wala is top user who has good similarity
#Niche wala is bottom user who doesn't have good similarity with user
# Get similar users + pivoted listens matrix for user 999
df_plot_c, user_c = get_sims_plot(
    999, df_pivot, user_similarity, df_test, recs_u2u_all, n_sim=25
)

# Plot heatmap for top 25 similar users
plot_heat(
    df_plot_c,
    f'{user_c} - Top 25 similar users',
    [c for c in df_plot_c.columns if c not in ['userId']],   # all artist columns
    'userId',
    w=75, h=100
)

# Same process for user 596
df_plot_c, user_c = get_sims_plot(
    596, df_pivot, user_similarity, df_test, recs_u2u_all, n_sim=25
)

plot_heat(
    df_plot_c,
    f'{user_c} - Top 25 similar users',
    [c for c in df_plot_c.columns if c not in ['userId']],
    'userId',
    w=75, h=60
)

# Cleanup
del df_plot_c, user_c
gc.collect()



#When it comes to User-to-User Collaborative Filtering it the the biggest problem wehn it comes to Collaborative filtering
#We have a package called as surpirse as it has K means implementation there we have little more hands to play out....
#One thing taht you can do when we build it we can go with 100 nearest neighbors,
#When you 35K atists and lakhs of users,expecting similarity between based on their artist listening might be not a great choice

# Free Up Memory
del user_similarity, df_pivot, df_train_imputed
gc.collect()

del df_train, df_test
gc.collect()

del users_to_rec, user_indices_to_rec, recs_u2u_all
gc.collect()

for i in range(3):
    gc.collect()



#the next point we are trying to understand, how do we mesaure if our recommendation could
#we can't compare thsi with supervised learning
#How do we know that our recommendation system is working well

""" ## Qualitative Testing"""

# A/B Testing
# Domain Experst to pickup some recommendation
# Qualitative is more of like feedback based Approach

"""## Quantitative Testing"""

#Generally we divide our data in train and test split
#so let's say user-1=[A1 A3 A7 A9 A13 ...]

#Recommendation=A7 A16 A14 A11
#Recommendation is not going to be one single recommendation and we generally make a list of recommendation
#We order them on some ranking like this is the top recommendation and theis is this etc
#Those are called as Top-N Recommendation

""" # It is judges by Two Matrix called as
 # 1)Precision at K
#  2)Recall at K
# k is the number of Top-N recommendation which we are going to give
"""

#Basically In Precsion at K
#me humlog Overlap dekhte hai and based on it we calculate the probability

#Basically In Recall at K
#me humlog total artist pe dekhte hai

#This is how we compute precision and recall at given matrix...

#Recall is , like intereted one, and the one which users has liked,
#out of these how many we picked

#There are also other qualitative mtrics like serendipity,etc
#if i haven't watched any partcular movie from any genre that doesn't mean that i don't like it so
#recommending a movie whcih is not my behavior and if i watch it,it's like invoking genre...

#Serendipity is like invoking interest by thworing a surprise conetnt that's not a part of behaviour but invoking interest
#that's why we can't use it for testing...
#we genrally use precisio at k and recall at k

#Lets do the train and test splt

"""# In Collaborative Filtering,there is two types of Techniques we use ,
#  1)User to Item Technique
#  2)Item to Item Technique
"""

#User-User (User-Item)
#here we find user based similarity,let's take user1 , now we will compute how similar user1 is with user2...

#Cosine Similarity
 #Euclidean Matrix

#Cosine Similarity se mera most similar user top pe aajayega

#User to user similarity or user to user neighbourhood uisng euclidean distance or something
#we find out for a given user who is the most similar user and based on the most similar user we picked out what this user is ot watched and oher watch and how much they like it
#and make recommendation based on it...

#ek baar wapis se padhna pdega yr ye...





"""# Item-to-Item"""

#In item to item we are going to play with Surpirse package instead of building it from scratch
df_i2i,users_i2i,artists_i2i=filter_lastfm_raw(df,user_sum,artist_sum,user_t=25,artist_t=25)
display(df_i2i.head())
print("Sample Users: ",users_i2i[:5])
print("Sample Artists: ",artists_i2i[2000:2020])
print(' ')

#This could still be a very arge dataset to handle so let's reduce it too...
n_user = 5000
n_artist = 5000
users_i2i = user_sum[:n_user]['userId'].values.tolist()
artist_i2i = artist_sum[:n_artist]['artistName'].values.tolist()

df_train_i2i = df_i2i[(df_i2i.split == 'Train')][['userId', 'artistName', 'listens']]
df_train_i2i = df_train_i2i[(df_train_i2i['userId'].isin(users_i2i)) & (df_train_i2i['artistName'].isin(artist_i2i))]

df_test_i2i = df_i2i[(df_i2i.split == 'Test')][['userId', 'artistName', 'listens']]
df_test_i2i = df_test_i2i[(df_test_i2i['userId'].isin(users_i2i)) & (df_test_i2i['artistName'].isin(artist_i2i))]

# df_train_i21['listens'] = np.log(df_train_i21['listens']+1)
# df_test_i21['listens'] = np.log(df_test_i21['listens']+1)

reader = Reader(rating_scale=(1, df_i2i['listens'].max()))
i2i_trainset = Dataset.load_from_df(df_train_i2i, reader)
i2i_trainset = i2i_trainset.build_full_trainset()

print('Sample records from item-item train set:')
display(df_train_i2i.head(2))

print(' ')
print('Sample records from item-item test set:')
display(df_test_i2i.head(2))

def get_elapsed(start,end):
    #Retur time elapsed between two times
    return f'{round(start-end/60,3)} Minutes'
def flatten(t):
    #Flatten a list of lists into a simple list
    return [item for sublist in t for item in sublist]

from time import time

start = time()
algo = KNNBasic(
    k=100,
    sim_options={'name': 'msd', 'user_based': False},
    verbose=True
)
algo.fit(i2i_trainset)
print(f"Fitting KNNBasic Item-Item Collaborative Filtering. {time() - start:.2f} seconds elapsed")

#When we we say user_based =True that means we are runnig User to User Collaborative Filtering using Euclidean Distance with the help of KNNBasic

#Now using this now check how our model is performing using Item to Item Collaborative filtering

from collections import defaultdict

def get_top_n(predictions, n=10):
    """Return the top-N recommendation for each user from a set of predictions."""
    top_n = defaultdict(list)

    # predictions is a list of Prediction objects: (uid, iid, true_r, est, details)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # Keep only the top-n for each user
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

def get_preds_i2i(user_c, algo, artists_u2u, df_train, df_test, n_rec=10):
    # Helper function to make predictions for sample user from item-item model
    avg_r = df_train['listens'].mean()
    y_true_train_c = df_train[df_train.userId==user_c]['artistName'].values.tolist()
    y_true_test_c = df_test[df_test.userId==user_c]['artistName'].values.tolist()
    item_to_pred_c = [(user_c, item, avg_r) for item in artists_u2u if item not in y_true_train_c]

    pred_c = algo.test(item_to_pred_c)
    top_c = get_top_n(pred_c, n=n_rec)
    top_c_df = pd.DataFrame([(k, t[0], v) for k, v in top_c.items() for t in v], columns=['userId', 'artistName', 'score'])
    top_c_df = top_c_df.sort_values('score', ascending=False).reset_index(drop=True)

    rec_req = [user_c, top_c_df[:n_rec], y_true_test_c]

    return rec_req

users_to_rec=users_i2i[:1000]
recs_i2i_all=[]
for i,u in enumerate(tqdm(users_to_rec)):
    rec_u=get_preds_i2i(u,algo,artists_i2i,df_train_i2i,df_test_i2i,n_rec=100)
    recs_i2i_all.append(rec_u)
y_true_i2i=[i[2] for i in recs_i2i_all]
y_pred_i2i=[i[1]['artistName'].values.tolist() for i in recs_i2i_all]

precision_i2i,recalls_i2i=pak(y_true_i2i,y_pred_i2i,k=50)

fix,ax=plt.subplots(1,2,figsize=(12,4))
fig.suptitle("Item Item CF",fontsize=16)
sns.distplot(precision_i2i,ax=ax[0])
sns.distplot(recalls_i2i,ax=ax[1])
ax[0].set_title('Precision by the User')
ax[1].set_title('Recall by User')
fig.tight_layout()

print("... Metrics for Item to Item Collaborative Filtering")
print("Average Precision: ",np.mean(precision_i2i))
print("Average Recall: ",np.mean(np.mean(recalls_i2i)))

#Maybe we need to do finetune it a lot if we want a descent output... #basically wapis se retrain krna pdega
#We talked about matrix factorization but how are we going to break thismatrix

#There are two options one is this schocastic graident decsent approach(weighted alternating List square)
#Other one is called as Alternative List Sqaure

#The cost function is like this - whole summqation(rui-rui^ )2 + regulariation parameter mtlb lambda(bi2 + bu2 + the weights means ||qi||2+ ||pi||2)

# Let us filter the data and build the MF model
df_als, users_als, artists_als = filter_lastfm_raw(df,user_sum,artist_sum,user_t=50,artist_t=50)

# We also need to convert strings to category format for modelling with implicit
df_als['userId'] = df_als['userId'].astype('category')
df_als['artistName'] = df_als['artistName'].astype('category')
df_als['user_ID'] = df_als['userId'].cat.codes
df_als['artist_ID'] = df_als['artistName'].cat.codes

display(df_als.head())
print('Sample users: ',users_als[:5])
print('Sample artists: ',artists_als[2037:2047])
print(' ')

df_als_train = df_als[df_als.split=='Train']

for i in range(1,3): gc.collect()

# Let us convert user-artist matrix into sparse format for modelling
sparse_item_user = sparse.csr_matrix((df_als_train['listens'].astype(float), (df_als_train['user_ID'], df_als_train['artist_ID'])))

pivot_user_item_for_item_user = True
if pivot_user_item_for_item_user:
    sparse_user_item = sparse_item_user.T.tocsr()
else:
    sparse_user_item = sparse.csr_matrix((df_als_train['listens'].astype(float), (df_als_train['artist_ID'],df_als_train['user_ID'])))

#We can also use NMF tooo but abhi ke liye yahi thik hai....
# Building the ALS implicit model (50,50 - works)
model = implicit.als.AlternatingLeastSquares(factors=50,regularization=0.1,iterations=50,num_threads=0)
df_als_conf = (sparse_item_user*10).astype('double')
model.fit(df_als_conf)
user_id_dict = df_als[['userId','user_ID']].drop_duplicates().reset_index(drop=True)
user_id_dict['trained_user'] = np.where(user_id_dict['userId'].isin(users_als),True,False)
artist_id_dict = df_als[['artist_ID','artistName']].drop_duplicates().reset_index(drop=True)

#Number of top recommendations for each user
n_rec=100
users_to_rec=list(user_id_dict[user_id_dict.trained_user]['user_ID'].values[:100])
als_rec_artists=model.recommend(users_to_rec,sparse_item_user[users_to_rec,:],n_rec,filter_already_liked_items=False)
y_pred_als=als_rec_artists[0].tolist()
y_true_als=list(df_als[(df_als.split=='Test')&(df_als.user_ID.isin(users_to_rec))].groupby('user_ID')['artist_ID'].apply(list))

precisions_als,recalls_als=pak(y_true_als,y_pred_als,k=50)

fix,ax=plt.subplots(1,2,figsize=(12,4))
fig.suptitle("Item Item CF",fontsize=16)
sns.distplot(precisions_als,ax=ax[0])
sns.distplot(recalls_als,ax=ax[1])
ax[0].set_title('Precision by the User')
ax[1].set_title('Recall by User')
fig.tight_layout()

print("... Metrics for Item to Item Collaborative Filtering")
print("Average Precision: ",np.mean(precisions_als))
print("Average Recall: ",np.mean(np.mean(recalls_als)))

#Slight Imporvement from Earlier, one we will have to tune the regularization factor...









